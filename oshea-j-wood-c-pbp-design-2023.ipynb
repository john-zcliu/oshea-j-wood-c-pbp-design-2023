{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a95475",
   "metadata": {},
   "source": [
    "# CONAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import datetime\n",
    "import gzip\n",
    "import pathlib\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import kaleido\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class PearsonAnalysis:\n",
    "    \"\"\"Performs peak//valley analysis of Pearson-time data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to the image data.\n",
    "    z_threshold: float\n",
    "        Flat cutoff for data in z dimension.\n",
    "    x_width_nm: float\n",
    "        Width of x dimension of image in nanometres.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        print('Loading data from \"{}\"...'.format(file_path))\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        # Pre-processing of peaks and valleys data to get all_data and all_clustered_data\n",
    "        self.raw_data = self.load_CONAN_as_2d_array(file_path) # Was previously: self.raw_data = numpy.reshape(raw_data_col, (-1, 4096))[::-1]\n",
    "        self.nresidues = len(self.raw_data)\n",
    "        self.pea_data = self.raw_data > 0\n",
    "        self.vly_data = self.raw_data < 0\n",
    "        self.pea_clustered_data, self.pea_cluster_num = scipy.ndimage.measurements.label(self.pea_data) # Generates an array of indexes on the features\n",
    "        self.vly_clustered_data, self.vly_cluster_num = scipy.ndimage.measurements.label(self.vly_data) # The clustered_data are input for several fucntions\n",
    "        self.vly_clustered_data = -self.vly_clustered_data\n",
    "        self.all_cluster_num = self.pea_cluster_num + self.vly_cluster_num\n",
    "        zip_clustered_data = numpy.dstack((self.pea_clustered_data, self.vly_clustered_data))\n",
    "        self.all_clustered_data = numpy.zeros((self.nresidues,self.nresidues))\n",
    "        for row in range(0,len(self.all_clustered_data)):\n",
    "            for column in range(0,len(self.all_clustered_data[row])):\n",
    "                self.all_clustered_data[row][column] = sum(zip_clustered_data[row][column])\n",
    "        \n",
    "        end_time = datetime.datetime.now()\n",
    "        time_taken = end_time - start_time\n",
    "        print(\"Data loaded in {} seconds.\".format(time_taken.seconds))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.all_cluster_num\n",
    "\n",
    "    def get_cluster_pixel_counts(self, min_pixel_count=0, max_pixel_count=None):\n",
    "        \"\"\"Generates a dictionary containing pixel counts for each cluster.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_pixel_count: int\n",
    "            Minimum number of pixels in cluster.\n",
    "        max_pixel_count: int\n",
    "            Maximum number of pixels in cluster.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cluster_pixel_dict: dict\n",
    "            Keys are the cluster identifier and values are the corresponding\n",
    "            pixel count.\n",
    "        \"\"\"\n",
    "        cluster_pixel_dict = Counter(self.all_clustered_data.flatten())\n",
    "        del cluster_pixel_dict[0]\n",
    "        if max_pixel_count is None:\n",
    "            max_pixel_count = max(cluster_pixel_dict.values())\n",
    "        return dict(\n",
    "            filter(\n",
    "                lambda x: min_pixel_count <= x[1] <= max_pixel_count,\n",
    "                cluster_pixel_dict.items(),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def get_cluster_heights(self, min_height=None, max_height=None):\n",
    "        \"\"\"Generates a dictionary containing height (Pearson coefficient) information for each cluster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_height: float\n",
    "            Minimum height of cluster.\n",
    "        max_height: float\n",
    "            Maximum height of cluster.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_height_dict: dict\n",
    "            Keys are the cluster identifier and values are the corresponding\n",
    "            height information in the format (mean, min, max, stddev, median).\n",
    "        \"\"\"\n",
    "        if max_height is None:\n",
    "            max_height = numpy.max(self.raw_data)\n",
    "        if min_height is None:\n",
    "            min_height = numpy.min(self.raw_data)\n",
    "        zipped_matrices = numpy.dstack((self.raw_data, self.all_clustered_data)) # dstack will \"stack\" two arrays on top of each other. Creates a 370x370 matrix where each point is a list of two values: Pearson and cluster \n",
    "        cluster_height_dict = {}\n",
    "        for row in zipped_matrices:\n",
    "            for (height, label) in row: # Each height, label pair corresponds to a residue pair\n",
    "                if min_height <= height <= max_height:\n",
    "                    label = int(label)\n",
    "                    if label:\n",
    "                        if label not in cluster_height_dict: # Make a new list for each cluster...\n",
    "                            cluster_height_dict[label] = []\n",
    "                        cluster_height_dict[label].append(height)# ... containing the Pearson values of all points within the cluster\n",
    "        return {\n",
    "            k: (numpy.mean(v), min(v), max(v), numpy.std(v), numpy.median(v))\n",
    "            for k, v in cluster_height_dict.items()\n",
    "        } # Then for each key only return the interesting data about the cluster\n",
    "    \n",
    "    def clustered_filtered_by_pixel_count(self, min_pixel_count, max_pixel_count=None):\n",
    "        \"\"\"Filters the cluster array by pixel count.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_pixel_count: int\n",
    "            Minimum number of pixels in cluster.\n",
    "        max_pixel_count: int\n",
    "            Maximum number of pixels in cluster.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_cluster_data: numpy.array\n",
    "            Updated cluster array (with cluster idices) removing clusters outside cut-off values.\n",
    "        \"\"\"\n",
    "        pixel_dict = self.get_cluster_pixel_counts(min_pixel_count, max_pixel_count) # This gives a pixel_dict with given cutoffs\n",
    "        \n",
    "        def if_in_pixel_dict(x):\n",
    "            if x in pixel_dict.keys(): # The vectorised function goes through each point in the clustered data. If the value is in the pixel_dict keys, then it keeps that value. If not, it replaces it ith zero.\n",
    "                return x\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        v_if_in_pixel_dict = numpy.vectorize(if_in_pixel_dict)\n",
    "        return v_if_in_pixel_dict(self.all_clustered_data)\n",
    "    \n",
    "    def get_maxmin_cluster_heights(self, min_height=None, max_height=None): \n",
    "        \"\"\"Generates a dictionary containing height (Pearson coefficient) information for each peak and valley.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_height: float\n",
    "            Minimum height of cluster.\n",
    "        max_height: float\n",
    "            Maximum height of cluster.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_height_dict: dict\n",
    "            Keys are the cluster identifier and values are the max for peaks and min for valleys.\n",
    "        \"\"\"\n",
    "        if max_height is None:\n",
    "            max_height = numpy.max(self.raw_data)\n",
    "        if min_height is None:\n",
    "            min_height = numpy.min(self.raw_data)\n",
    "        zipped_matrices = numpy.dstack((self.raw_data, self.all_clustered_data))\n",
    "        cluster_height_dict = {}\n",
    "        for row in zipped_matrices:\n",
    "            for (height, label) in row:\n",
    "                label = int(label)\n",
    "                if label:\n",
    "                    if label not in cluster_height_dict:\n",
    "                        cluster_height_dict[label] = []\n",
    "                    cluster_height_dict[label].append(height)\n",
    "        cluster_maxmin_height_dict = {}\n",
    "        for k, v in cluster_height_dict.items(): # Get the max or min values at the end of the value list.\n",
    "            if k > 0: # For peaks get the max value\n",
    "                cluster_maxmin_height_dict[k] = (max(v))\n",
    "            if k < 0: # For valleys get the min value\n",
    "                cluster_maxmin_height_dict[k] = (min(v))\n",
    "        return cluster_maxmin_height_dict\n",
    "    \n",
    "#    def clustered_filtered_by_magnitude(self, valley_cutoff, peak_cutoff):\n",
    "#        \"\"\"Filters the cluster array by pixel count.#\n",
    "\n",
    "#        Parameters\n",
    "#        ----------\n",
    "#        min_pixel_count: int\n",
    "#            Minimum number of pixels in cluster.\n",
    "#        max_pixel_count: int\n",
    "#            Maximum number of pixels in cluster.##\n",
    "\n",
    "#        Returns\n",
    "#        -------\n",
    "#        filtered_cluster_data: numpy.array\n",
    "#            Updated cluster array (with cluster idices) removing clusters outside cut-off values.\n",
    "#        \"\"\"\n",
    "#        maxmin_dict = self.get_maxmin_cluster_heights(min_pixel_count, max_pixel_count) # This gives a pixel_dict with given cutoffs\n",
    "#    \n",
    "#maxmin_dict = self.get_maxmin_cluster_heights()\n",
    "#maxmin_filtered_dict = dict(\n",
    "#            filter(\n",
    "#                lambda x: 0.8 <= abs(x[1]) <= 1,\n",
    "#                maxmin_dict.items(),\n",
    "#            )\n",
    "#    )\n",
    "    \n",
    "    def array_from_clustered_dict(self, clustered_dict):\n",
    "        \"\"\"\n",
    "        Creates a numpy array from a dictionary of cluster indices.\n",
    "        Returns an arra of raw Pearson data values far all points of these clusters.\n",
    "        \"\"\"\n",
    "        \n",
    "        array = numpy.zeros((self.nresidues,self.nresidues))\n",
    "        for row in range(0,len(array)):\n",
    "            for col in range(0,len(array)):\n",
    "                if self.all_clustered_data[row][col] in clustered_dict.keys():\n",
    "                    array[row][col] = self.raw_data[row][col]\n",
    "        return array\n",
    "    \n",
    "    def merge_cluster_dicts(major_dict, minor_dict):\n",
    "        merged_dict = {}\n",
    "        for k, v in major_dict.items(): # for keys, values in items of major dict...\n",
    "            merged_dict[k] = (v, minor_dict[k]) # merged dict has the value of the major dict AND the minor dict, in form k : (\n",
    "        return merged_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def load_CONAN_as_2d_array(file_path):\n",
    "        \"\"\"Creates a numpy array from a CONAN Pearson-time data file.\"\"\"\n",
    "        with open(file_path, \"r\") as inf:\n",
    "            #array = None # Need to input how many residues there are\n",
    "            number_of_residues = None\n",
    "            for line in inf.readlines()[::-1]: # Go from last residue first, so that the 1,1 point is in the bottom left\n",
    "                try:\n",
    "                    row, column, pear = line.split() # Get the columns\n",
    "                    if not number_of_residues:\n",
    "                        number_of_residues = int(row) # Get the index of the final residue, use it to define the dimensions of raw_data\n",
    "                        data = numpy.zeros((number_of_residues,number_of_residues))\n",
    "                    data[int(row)-1][int(column)-1] = float(pear) # Put the value in the correct place\n",
    "                except:\n",
    "                    pass # Pass empty rows that would cause error\n",
    "        return data # Get the 1,1 index at the bottom left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conan_graph(inpPA, title=\"\", meanP=0, medP=0, mxmnP=0, stdvP=0, res_interest=[], save=0):\n",
    "    pa1 = PearsonAnalysis(inpPA)\n",
    "    \n",
    "    # Plot matrix per cluster so each cluster can be labelled with the folowing hover-information:\n",
    "    # peak/valley max/min percentile, peak/valley mean percentile, peak/valley median percentile, stdev percentile\n",
    "    \n",
    "    # Get a dictionary with k = cluster and v = percentile ranks + list of residues\n",
    "\n",
    "    cluster_info_dict = pa1.get_cluster_heights() # clust: (numpy.mean(v), min(v), max(v), numpy.std(v), numpy.median(v))\n",
    "    maxmin_dict = pa1.get_maxmin_cluster_heights() # clust: max if peak or min if valley\n",
    "    cent_dict = {}\n",
    "\n",
    "    for clust in cluster_info_dict.keys():\n",
    "        cent_dict[clust] = [] # clust: [mean, median, max/min, std]\n",
    "\n",
    "    # Get mean percentile\n",
    "    # Get ranking of indices\n",
    "    pea_rnk = sorted([v[0] for v in cluster_info_dict.values() if v[0] > 0], reverse=True) # Must be reversed so that index() returns the highest index of the queried value\n",
    "    vly_rnk = sorted([v[0] for v in cluster_info_dict.values() if v[0] < 0], reverse=False)\n",
    "\n",
    "    # For each cluster, find how many values it is greater than or equal to\n",
    "    for clust, v in cluster_info_dict.items():\n",
    "        if clust > 0:\n",
    "            cent_dict[clust].append(round((len(pea_rnk)-pea_rnk.index(v[0])) / len(pea_rnk) * 100, 2))\n",
    "        if clust < 0:\n",
    "            cent_dict[clust].append(round((len(vly_rnk)-vly_rnk.index(v[0])) / len(vly_rnk) * 100, 2))\n",
    "\n",
    "    # Get median percentile\n",
    "    # Get ranking of indices\n",
    "    pea_rnk = sorted([v[4] for v in cluster_info_dict.values() if v[4] > 0], reverse=True) # Must be reversed so that index() returns the highest index of the queried value\n",
    "    vly_rnk = sorted([v[4] for v in cluster_info_dict.values() if v[4] < 0], reverse=False)\n",
    "\n",
    "    # For each cluster, find how many values it is greater than or equal to\n",
    "    for clust, v in cluster_info_dict.items():\n",
    "        if clust > 0:\n",
    "            cent_dict[clust].append(round((len(pea_rnk)-pea_rnk.index(v[4])) / len(pea_rnk) * 100, 2))\n",
    "        if clust < 0:\n",
    "            cent_dict[clust].append(round((len(vly_rnk)-vly_rnk.index(v[4])) / len(vly_rnk) * 100, 2))\n",
    "\n",
    "    # Get max/min percentile\n",
    "    # Get ranking of indices\n",
    "    pea_rnk = sorted([v for v in maxmin_dict.values() if v > 0], reverse=True) # Must be reversed so that index() returns the highest index of the queried value\n",
    "    vly_rnk = sorted([v for v in maxmin_dict.values() if v < 0], reverse=False)\n",
    "\n",
    "    # For each cluster, find how many values it is greater than or equal to\n",
    "    for clust, v in maxmin_dict.items():\n",
    "        if clust > 0:\n",
    "            cent_dict[clust].append(round((len(pea_rnk)-pea_rnk.index(v)) / len(pea_rnk) * 100, 2))\n",
    "        if clust < 0:\n",
    "            cent_dict[clust].append(round((len(vly_rnk)-vly_rnk.index(v)) / len(vly_rnk) * 100, 2))\n",
    "\n",
    "    # Get stdev percentile\n",
    "    # Get ranking of indices\n",
    "    rnk = sorted([v[3] for v in cluster_info_dict.values()], reverse=True) # Must be reversed so that index() returns the highest index of the queried value  \n",
    "\n",
    "    # For each cluster, find how many values it is greater than or equal to\n",
    "    for clust, v in cluster_info_dict.items():\n",
    "        cent_dict[clust].append(round((len(rnk) - rnk.index(v[3])) / len(rnk) * 100,2))\n",
    "\n",
    "    # make percentile arrays\n",
    "    cluster_dict = pa1.all_clustered_data\n",
    "    mean_percent_array = numpy.zeros((370,370))\n",
    "    median_percent_array = numpy.zeros((370,370))\n",
    "    maxmin_percent_array = numpy.zeros((370,370))\n",
    "    stdev_percent_array = numpy.zeros((370,370))\n",
    "    for row in range(0, len(cluster_dict)):\n",
    "        for col in range(0, len(cluster_dict[row])):\n",
    "            if cluster_dict[row][col]:\n",
    "                mean_percent_array[row][col] = cent_dict[cluster_dict[row][col]][0]\n",
    "                median_percent_array[row][col] = cent_dict[cluster_dict[row][col]][1]\n",
    "                maxmin_percent_array[row][col] = cent_dict[cluster_dict[row][col]][2]\n",
    "                stdev_percent_array[row][col] = cent_dict[cluster_dict[row][col]][3]\n",
    "    \n",
    "    # Filter by percentile. > Nth for all metrics\n",
    "    filt_by_cent = numpy.zeros((370,370))\n",
    "    percent_arrays = [mean_percent_array, median_percent_array, maxmin_percent_array, stdev_percent_array]\n",
    "    for row in range(0, len(filt_by_cent)):\n",
    "        for col in range(0, len(filt_by_cent[row])):\n",
    "            if mean_percent_array[row][col] > meanP:\n",
    "                if median_percent_array[row][col] > medP:\n",
    "                    if maxmin_percent_array[row][col] > mxmnP:\n",
    "                        if stdev_percent_array[row][col] > stdvP:\n",
    "                            filt_by_cent[row][col] = pa1.raw_data[row][col]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.select_coloraxes\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=filt_by_cent,\n",
    "        colorscale=\"PuOr\",\n",
    "        customdata=numpy.moveaxis([mean_percent_array, median_percent_array, maxmin_percent_array, stdev_percent_array], 0,-1),\n",
    "        hovertemplate=\"   mean %ile: %{customdata[0]}<br>   median %ile: %{customdata[1]}<br>   maxmin %ile: %{customdata[2]}<br>   stdev %ile: %{customdata[3]}\"\n",
    "        )\n",
    "             )\n",
    "    #res_interest = [170,169,171,335,348,346] # This defines the RANKING of the lines \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sorted(res_interest), # This must be sorted, otherwise the heatmap of the bars will be done by ascending order inserted of the RANKING\n",
    "        y=[pa1.nresidues for value in res_interest],\n",
    "        orientation=\"v\",\n",
    "        width=2,\n",
    "        marker={\"color\": res_interest, \"colorscale\": \"Viridis_r\"},\n",
    "        opacity=0.6\n",
    "\n",
    "        )\n",
    "             )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=700,\n",
    "        height=700\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists(\"images\"):\n",
    "            os.mkdir(\"images\")\n",
    "        fig.write_image(f\"images/{title}.svg\")\n",
    "        fig.write_image(f\"images/{title}.png\", )\n",
    "    # Save filtered array in the same format as CONAN peason_data.dat output, so it can be accessed by PearsonAnalysis() in the same way\n",
    "    with open(f\"images/{title}.dat\", \"w\") as ouf:\n",
    "        for row in range(0, len(filt_by_cent)):\n",
    "            for col in range(0, len(filt_by_cent[row])):\n",
    "                line = f\"{row+1}  {col+1}  {filt_by_cent[row][col]}\\n\"\n",
    "                ouf.write(line) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa124837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average out all CONAN plots that have the openning movement (so excluding 02 and 03) for PearsonAnalysis()\n",
    "\n",
    "# Get raw CONAN data into a list of lists\n",
    "sims = [\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "raw_list = []\n",
    "for sim in sims:\n",
    "    path = f\"CONAN/conan1_{sim}/aggregate/pearson_data.dat\"\n",
    "    if os.path.exists(path):\n",
    "        #print(path)\n",
    "        pa = PearsonAnalysis(path)\n",
    "        raw_list.append(pa.raw_data)\n",
    "print(raw_list)\n",
    "# Get an array of mean values across the simulations for each array point\n",
    "mean_array = numpy.zeros((370,370))\n",
    "for row in range(0, len(mean_array)):\n",
    "    for col in range(0, len(mean_array[row])):\n",
    "        mean_array[row][col] = numpy.mean([raw[row][col] for raw in raw_list])\n",
    "print(mean_array)\n",
    "# Save mean array in the same format as CONAN peason_data.dat output, so it can be accessed by PearsonAnalysis() in the same way\n",
    "with open(\"CONAN/2022-09-13_mbp_all_sim_conans_mean.dat\", \"w\") as ouf:\n",
    "    for row in range(0, len(mean_array)):\n",
    "        for col in range(0, len(mean_array[row])):\n",
    "            line = f\"{row+1}  {col+1}  {mean_array[row][col]}\\n\"\n",
    "            ouf.write(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab8409",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conan_graph(\"CONAN/2022-09-13_mbp_all_sim_conans_mean.dat\", title=\"Unfiltered mean\", save=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conan_graph(\"CONAN/2022-09-13_mbp_all_sim_conans_mean.dat\", meanP=99, medP=99, mxmnP=99, title=\"MBP_all_>99th\", save=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fec09",
   "metadata": {},
   "source": [
    "# dDIHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the crystal structure dihedrals with the dihedrals of the last 50 ns of the 100 ns apo and the 200 ns holo simulations\n",
    "# Get the dDIHE. HOLO, last 50 ns of all 10. APO, last 50 ns of all excluding 2 and 3 (in which the opening transition doesn't take place)\n",
    "\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "output_dict = {}\n",
    "state = \"apo\"\n",
    "apo_top_path = \"Simulations/Apo/1anf_malremoved_t3p.parm7\"\n",
    "\n",
    "for sim in [1,4,5,6,7,8,9,10]:\n",
    "    print(f\"{state} sim {sim}...\")\n",
    "    apo_dcd_path = f\"Simulations/Apo/npt_production_{sim:02d}.dcd\"\n",
    "    \n",
    "    traj = md.load_dcd(apo_dcd_path, top=apo_top_path, stride=5) # Load a frame for every ns of the simulation\n",
    "    last_50 = traj[-50:] # Trajectory object of the last 50 frames\n",
    "\n",
    "    calpha_numbers = [a.index for a in traj.topology.atoms if a.name == 'CA'] # For the atoms in topology, make a list of the atoms that are CA\n",
    "    fours = [calpha_numbers[calpha_numbers.index(at)-1:calpha_numbers.index(at)+3] for at in calpha_numbers[1:-2]] # Get a list of lists of 4 atoms\n",
    "    all_radians = md.compute_dihedrals(last_50, fours) # Compute radians for all positions at all time points\n",
    "\n",
    "    for i in range(0, len(fours)): # Each item in fours is a list of four atom indicies. There is an item for each residue that has 1 residue behind and 2 in front\n",
    "        res = str(traj.topology.atom(fours[i][1]))[0:-3] # The str() contents will be RESXX-CA where RES is the 3 letter code, XX is the residue number, CA is indicating that it is the calpha. res = [0:-3] of this string, getting rid of the \"-CA\" leaving only \"RESXX\"\n",
    "\n",
    "        if res not in output_dict.keys(): # On the first pass, create the dictionary and fill with information that applies to all states:\n",
    "            output_dict[res] = dict() # Create a dictionary of information for the residue corresponding to this atom\n",
    "            output_dict[res][\"dDIHE_mean_of_means\"] = float()\n",
    "\n",
    "        if state not in output_dict[res].keys(): # Create dictionary for each state (apo or holo)\n",
    "            output_dict[res][state] = dict()\n",
    "            output_dict[res][state][\"DIHE_means\"] = []\n",
    "            output_dict[res][state][\"DIHE_mean_of_means\"] = float()\n",
    "\n",
    "        if f\"{sim:02d}\" not in output_dict[res][state].keys(): # For each new simulation:\n",
    "            output_dict[res][state][f\"{sim:02d}\"] = dict()\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"] = [] # The dihedral angle at this position for each nanosecond of this simulation.\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_mean\"] = float()\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_stdv\"] = float()\n",
    "\n",
    "        for ns in all_radians:\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"].append(ns[i]) # Get the dihedral angles for this simulation at this position for each nanosecond\n",
    "\n",
    "        output_dict[res][state][f\"{sim:02d}\"][\"dihe_mean\"] = np.mean(output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"])\n",
    "        output_dict[res][state][f\"{sim:02d}\"][\"dihe_stdv\"] = np.std(output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"])\n",
    "        output_dict[res][state][\"DIHE_means\"].append(output_dict[res][state][f\"{sim:02d}\"][\"dihe_mean\"])\n",
    "\n",
    "state = \"holo\"\n",
    "bou_top_path = \"Simulations/Holo/1anf_mal_t3p.parm7\"\n",
    "\n",
    "for sim in range(1,11):\n",
    "    print(f\"{state} sim {sim}...\")\n",
    "    bou_dcd_path = f\"Simulations/Holo/npt_production_{sim:02d}.dcd\"\n",
    "    traj = md.load_dcd(bou_dcd_path, top=bou_top_path, stride=5) # Load a frame for every ns of the simulation\n",
    "    last_50 = traj[-50:] # Trajectory object of the last 50 frames\n",
    "    \n",
    "    calpha_numbers = [a.index for a in traj.topology.atoms if a.name == 'CA'] # For the atoms in topology, make a list of the atoms that are CA\n",
    "    fours = [calpha_numbers[calpha_numbers.index(at)-1:calpha_numbers.index(at)+3] for at in calpha_numbers[1:-2]] # Get a list of lists of 4 atoms\n",
    "    all_radians = md.compute_dihedrals(last_50, fours) # Compute radians for all positions at all time points\n",
    "    \n",
    "    for i in range(0, len(fours)): # Each item in fours is a list of four atom indicies. There is an item for each residue that has 1 residue behind and 2 in front\n",
    "        res = str(traj.topology.atom(fours[i][1]))[0:-3] # The str() contents will be RESXX-CA where RES is the 3 letter code, XX is the residue number, CA is indicating that it is the calpha. res = [0:-3] of this string, getting rid of the \"-CA\" leaving only \"RESXX\"\n",
    "\n",
    "        if res not in output_dict.keys(): # On the first pass, create the dictionary and fill with information that applies to all states:\n",
    "            output_dict[res] = dict() # Create a dictionary of information for the residue corresponding to this atom\n",
    "            output_dict[res][\"dDIHE_mean_of_means\"] = float()\n",
    "\n",
    "        if state not in output_dict[res].keys(): # Create dictionary for each state (apo or holo)\n",
    "            output_dict[res][state] = dict()\n",
    "            output_dict[res][state][\"DIHE_means\"] = []\n",
    "            output_dict[res][state][\"DIHE_mean_of_means\"] = float()\n",
    "\n",
    "        if f\"{sim:02d}\" not in output_dict[res][state].keys(): # For each new simulation:\n",
    "            output_dict[res][state][f\"{sim:02d}\"] = dict()\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"] = [] # The dihedral angle at this position for each nanosecond of this simulation.\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_mean\"] = float()\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_stdv\"] = float()\n",
    "\n",
    "        for ns in all_radians:\n",
    "            output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"].append(ns[i]) # Get the dihedral angles for this simulation at this position for each nanosecond\n",
    "\n",
    "        output_dict[res][state][f\"{sim:02d}\"][\"dihe_mean\"] = np.mean(output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"])\n",
    "        output_dict[res][state][f\"{sim:02d}\"][\"dihe_stdv\"] = np.std(output_dict[res][state][f\"{sim:02d}\"][\"dihe_per_ns\"])\n",
    "        output_dict[res][state][\"DIHE_means\"].append(output_dict[res][state][f\"{sim:02d}\"][\"dihe_mean\"])\n",
    "\n",
    "for state in [\"apo\", \"holo\"]:\n",
    "    for res in output_dict.keys():\n",
    "        output_dict[res][state][\"DIHE_mean_of_means\"] = np.mean(output_dict[res][state][\"DIHE_means\"])\n",
    "    \n",
    "for res in output_dict.keys():    \n",
    "    change = output_dict[res][\"apo\"][\"DIHE_mean_of_means\"] - output_dict[res][\"holo\"][\"DIHE_mean_of_means\"]\n",
    "    \n",
    "    if -np.pi <= change <= np.pi:\n",
    "        output_dict[res][\"dDIHE_mean_of_means\"] = change\n",
    "    if change > np.pi:\n",
    "        output_dict[res][\"dDIHE_mean_of_means\"] = 2*np.pi - (change)\n",
    "    if change < -np.pi:\n",
    "        output_dict[res][\"dDIHE_mean_of_means\"] = 2*np.pi + (change)\n",
    "\n",
    "# write results to CSV\n",
    "import csv\n",
    "    \n",
    "with open(\"dDIHE/2022-07-27_maltosePBP_sim_exc2,3_dDIHE.csv\", \"w\", encoding = \"latin\") as outf:\n",
    "    csv_columns = ()\n",
    "    csv_data = [[\"residue identified\", \"residue number\", \"dDIHE\"]]\n",
    "    \n",
    "    for res in output_dict.keys():\n",
    "        csv_data.append([]) # want residue identifier, residue number, dDIHE\n",
    "        csv_data[-1].append(res) # Write a list of lists where each entry is a row\n",
    "        csv_data[-1].append(res[3:])\n",
    "        csv_data[-1].append(output_dict[res][\"dDIHE_mean_of_means\"])\n",
    "        \n",
    "        \n",
    "    writer= csv.writer(outf)\n",
    "    for i in range(0,len(csv_data)):\n",
    "        writer.writerow(csv_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76938d",
   "metadata": {},
   "source": [
    "# RMSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dRMSF data\n",
    "\n",
    "output = {}\n",
    "\n",
    "import csv\n",
    "import scipy.stats\n",
    "import os\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis import rms, align\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sim = 0\n",
    "\n",
    "apo_top = \"Simulations/Apo/1anf_malremoved_t3p.parm7\"\n",
    "apo_dcd = f\"Simulations/Apo/npt_production_{sim:02d}.dcd\"    \n",
    "hol_top = \"Simulations/Holo/1anf_mal_t3p.parm7\"\n",
    "hol_dcd = f\"Simulations/Holo/simulation_{sim:02d}/npt_production_{sim:02d}.dcd\"\n",
    "\n",
    "state_dict = {\"apo\": {\"top\" : apo_top, \"dcd\" : apo_dcd}, \"hol\": {\"top\" : hol_top, \"dcd\" : hol_dcd}}\n",
    "\n",
    "output = {}\n",
    "\n",
    "for state in [\"hol\", \"apo\"]:\n",
    "    output[state] = {}\n",
    "    print(state)\n",
    "    if state == \"hol\":\n",
    "        for sim in [1,2,3,4,5,6,7,8,9,10]:\n",
    "            print(sim)\n",
    "            # Get the paths to the topologies and dcds\n",
    "            apo_top = \"Simulations/Apo/1anf_malremoved_t3p.parm7\"\n",
    "            apo_dcd = f\"Simulations/Apo/npt_production_{sim:02d}.dcd\"    \n",
    "            hol_top = \"Simulations/Holo/1anf_mal_t3p.parm7\"\n",
    "            hol_dcd = f\"Simulations/Holo/simulation_{sim:02d}/npt_production_{sim:02d}.dcd\"\n",
    "            state_dict = {\"apo\": {\"top\" : apo_top, \"dcd\" : apo_dcd}, \"hol\": {\"top\" : hol_top, \"dcd\" : hol_dcd}}\n",
    "\n",
    "            u = mda.Universe(state_dict[state][\"top\"],state_dict[state][\"dcd\"]) # reads as: u = mda.Universe(apo_top,apo_dcd)\n",
    "\n",
    "            average = align.AverageStructure(u, u, select='protein and name CA',\n",
    "                                         ref_frame=0).run()\n",
    "            ref = average.results.universe\n",
    "\n",
    "            aligner = align.AlignTraj(u, ref,\n",
    "                                  select='protein and name CA',\n",
    "                                  in_memory=True).run() # Align the entire trajectory to the reference (average) structure of the trajectory\n",
    "            print(f\"calculating rmsf {state} {sim}\")\n",
    "            c_alphas = u.select_atoms('protein and name CA') # get a c_alphas object from our reference-aligned universe\n",
    "            R = rms.RMSF(c_alphas).run() # Calculate the RMSF of the c-alphas\n",
    "\n",
    "\n",
    "            if \"per_sim\" not in output[state].keys():\n",
    "                output[state][\"per_sim\"] = {}\n",
    "            if f\"{sim:02d}\" not in output[state][\"per_sim\"].keys():\n",
    "                output[state][\"per_sim\"][f\"{sim:02d}\"] = R.results.rmsf\n",
    "        \n",
    "        # Define the list of residues\n",
    "        res_list =  [c_alphas.resnames[i]+str(int(c_alphas.resnums[i]-1)) for i in range(0,len(c_alphas.resnames))]\n",
    "        for res in res_list:\n",
    "            # Get the rmsf values for this residue across all simulations\n",
    "            rmsf_per_res = []\n",
    "            for sim in output[state][\"per_sim\"].keys():\n",
    "                rmsf_per_res.append(output[state][\"per_sim\"][f\"{sim}\"][res_list.index(res)])\n",
    "            if \"per_res\" not in output[state].keys():\n",
    "                output[state][\"per_res\"] = {}\n",
    "            output[state][\"per_res\"][res] = {}\n",
    "            output[state][\"per_res\"][res][\"rmsf_values\"] = rmsf_per_res\n",
    "            output[state][\"per_res\"][res][\"mean\"] = scipy.stats.tmean(output[state][\"per_res\"][res][\"rmsf_values\"])\n",
    "            output[state][\"per_res\"][res][\"SD\"] = scipy.stats.tstd(output[state][\"per_res\"][res][\"rmsf_values\"])\n",
    "            output[state][\"per_res\"][res][\"SEM\"] = scipy.stats.sem(output[state][\"per_res\"][res][\"rmsf_values\"])\n",
    "\n",
    "    if state == \"apo\":\n",
    "        for sim in [1,4,5,6,7,8,9,10]:\n",
    "            print(sim)\n",
    "            # Get the paths to the topologies and dcds\n",
    "            apo_top = \"Simulations/Apo/1anf_malremoved_t3p.parm7\"\n",
    "            apo_dcd = f\"Simulations/Apo/npt_production_{sim:02d}.dcd\"    \n",
    "            hol_top = \"Simulations/Holo/1anf_mal_t3p.parm7\"\n",
    "            hol_dcd = f\"Simulations/Holo/simulation_{sim:02d}/npt_production_{sim:02d}.dcd\"\n",
    "            state_dict = {\"apo\": {\"top\" : apo_top, \"dcd\" : apo_dcd}, \"hol\": {\"top\" : hol_top, \"dcd\" : hol_dcd}}\n",
    "\n",
    "            u = mda.Universe(state_dict[state][\"top\"],state_dict[state][\"dcd\"]) # reads as: u = mda.Universe(apo_top,apo_dcd)\n",
    "\n",
    "            average = align.AverageStructure(u, u, select='protein and name CA',\n",
    "                                         ref_frame=0).run()\n",
    "            ref = average.results.universe\n",
    "\n",
    "            aligner = align.AlignTraj(u, ref,\n",
    "                                  select='protein and name CA',\n",
    "                                  in_memory=True).run() # Align the entire trajectory to the reference (average) structure of the trajectory\n",
    "            print(f\"calculating rmsf {state} {sim}\")\n",
    "            c_alphas = u.select_atoms('protein and name CA') # get a c_alphas object from our reference-aligned universe\n",
    "            R = rms.RMSF(c_alphas).run() # Calculate the RMSF of the c-alphas\n",
    "\n",
    "\n",
    "            if \"per_sim\" not in output[state].keys():\n",
    "                output[state][\"per_sim\"] = {}\n",
    "            if f\"{sim:02d}\" not in output[state][\"per_sim\"].keys():\n",
    "                output[state][\"per_sim\"][f\"{sim:02d}\"] = R.results.rmsf\n",
    "        \n",
    "        # Define the list of residues\n",
    "        res_list =  [c_alphas.resnames[i]+str(int(c_alphas.resnums[i]-1)) for i in range(0,len(c_alphas.resnames))]\n",
    "        for res in res_list:\n",
    "            # Get the rmsf values for this residue across all simulations\n",
    "            rmsf_per_res = []\n",
    "            for sim in output[state][\"per_sim\"].keys():\n",
    "                rmsf_per_res.append(output[state][\"per_sim\"][f\"{sim}\"][res_list.index(res)])\n",
    "            if \"per_res\" not in output[state].keys():\n",
    "                output[state][\"per_res\"] = {}\n",
    "            output[state][\"per_res\"][res] = {}\n",
    "            output[state][\"per_res\"][res][\"rmsf_values\"] = rmsf_per_res\n",
    "            output[state][\"per_res\"][res][\"mean\"] = scipy.stats.tmean(output[state][\"per_res\"][res][\"rmsf_values\"])\n",
    "            output[state][\"per_res\"][res][\"SD\"] = scipy.stats.tstd(output[state][\"per_res\"][res][\"rmsf_values\"])\n",
    "            output[state][\"per_res\"][res][\"SEM\"] = scipy.stats.sem(output[state][\"per_res\"][res][\"rmsf_values\"])\n",
    "\n",
    "output[\"dRMSF\"] = {}\n",
    "for res in res_list:\n",
    "    output[\"dRMSF\"][res] = output[\"hol\"][\"per_res\"][res][\"mean\"] - output[\"apo\"][\"per_res\"][res][\"mean\"]\n",
    "\n",
    "# Record all RMSF data into csvs\n",
    "for state in state_dict.keys():\n",
    "    with open(f\"dRMSF/2022-08-01_{state}_rmsf_per_residue.csv\",\"w\",encoding=\"latin\") as outf:\n",
    "        writer = csv.writer(outf)\n",
    "        res_list =  [c_alphas.resnames[i]+str(int(c_alphas.resnums[i]-1)) for i in range(0,len(c_alphas.resnames))]\n",
    "        headers = res_list\n",
    "        headers.insert(0,\"res_id\")\n",
    "        writer.writerow(headers)\n",
    "        for sim in output[state][\"per_sim\"].keys():\n",
    "            row = list(output[state][\"per_sim\"][sim])\n",
    "            row.insert(0,sim)\n",
    "            writer.writerow(row)\n",
    "                \n",
    "# Record dRMSF        \n",
    "with open(\"dRMSF/2023-02-08_drmsf_per_residue_excl_apo2,3.csv\",\"w\",encoding=\"latin\") as outf:\n",
    "    header = []\n",
    "    apo = []\n",
    "    holo = []\n",
    "    drmsf = []\n",
    "    for res in output[\"dRMSF\"].keys():\n",
    "        header.append(res)\n",
    "        apo.append(output[\"apo\"][\"per_res\"][res][\"mean\"])\n",
    "        holo.append(output[\"hol\"][\"per_res\"][res][\"mean\"])\n",
    "        drmsf.append(output[\"dRMSF\"][res])\n",
    "    header.insert(0,\"residue\")\n",
    "    apo.insert(0,\"apo\")\n",
    "    holo.insert(0,\"holo\")\n",
    "    drmsf.insert(0,\"dRMSF\")\n",
    "    writer = csv.writer(outf)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(apo)\n",
    "    writer.writerow(holo)\n",
    "    writer.writerow(drmsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe852a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
